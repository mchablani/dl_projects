{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use content loss to create a super-resolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.insert(0, '../util')\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpath = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_lr = bcolz.open(dpath+'trn_resized_72.bc')[:]\n",
    "arr_hr = bcolz.open(dpath+'trn_resized_288.bc')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = {'verbose': 0, 'callbacks': [TQDMNotebookCallback(leave_inner=True)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=(2,2), mode='same', act=True):\n",
    "    x = Convolution2D(filters, size, size, subsample=stride, border_mode=mode)(x)\n",
    "    x = BatchNormalization(mode=2)(x)\n",
    "    return Activation('relu')(x) if act else x\n",
    "\n",
    "def res_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, (1,1))\n",
    "    x = conv_block(x, nf, 3, (1,1), act=False)\n",
    "    return merge([x, ip], mode='sum')\n",
    "\n",
    "def deconv_block(x, filters, size, shape, stride=(2,2)):\n",
    "    x = Deconvolution2D(filters, size, size, subsample=stride, \n",
    "        border_mode='same', output_shape=(None,)+shape)(x)\n",
    "    x = BatchNormalization(mode=2)(x)\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def up_block(x, filters, size):\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "    x = Convolution2D(filters, size, size, border_mode='same')(x)\n",
    "    x = BatchNormalization(mode=2)(x)\n",
    "    return Activation('relu')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This model here is using the previously defined blocks to encode a low resolution image and \n",
    "# then upsample it to match the same image in high resolution.\n",
    "\n",
    "inp=Input(arr_lr.shape[1:])\n",
    "x=conv_block(inp, 64, 9, (1,1))\n",
    "for i in range(4): x=res_block(x)\n",
    "x=up_block(x, 64, 3)\n",
    "x=up_block(x, 64, 3)\n",
    "x=Convolution2D(3, 9, 9, activation='tanh', border_mode='same')(x)\n",
    "outp=Lambda(lambda x: (x+1)*127.5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shp = \n",
    "shp = outp.shape[1:]\n",
    "\n",
    "preproc = lambda x: (x-imagenet_mean)[:,:,:,::-1]\n",
    "deproc = lambda x,s: np.clip(x.reshape(s)[:, :, :, ::-1] + imagenet_mean, 0, 255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of training this network is almost exactly the same as training the pixels from our previous implementations. The idea here is we're going to feed two images to Vgg16 and compare their convolutional outputs at some layer. These two images are the target image (which in our case is the same as the original but at higher resolution), and the output of the previous network we just defined, which we hope will learn to output a high resolution image. \n",
    "\n",
    "The key then is to train this other network to produce an image that minimizes the loss between the outputs of some convolutional layer in Vgg16 (which the paper refers to as \"perceptual loss\"). In doing so, we are able to train a network that can upsample an image and recreate the higher resolution details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_inp=Input(shp)\n",
    "vgg= VGG16(include_top=False, input_tensor=Lambda(preproc)(vgg_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we only want to learn the \"upsampling network\", and are just using VGG to calculate the loss function, \n",
    "# we set the Vgg layers to not be trainable.\n",
    "for l in vgg.layers: l.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference in training for super resolution is the loss function. We use what's known as a perceptual loss function (which is simply the content loss for some layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outp(m, ln): return m.get_layer(f'block{ln}_conv1').output\n",
    "\n",
    "vgg_content = Model(vgg_inp, [get_outp(vgg, o) for o in [1,2,3]])\n",
    "\n",
    "vgg1 = vgg_content(vgg_inp)\n",
    "vgg2 = vgg_content(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_sqr_b(diff): \n",
    "    dims = list(range(1,K.ndim(diff)))\n",
    "    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=[0.1, 0.8, 0.1]\n",
    "def content_fn(x): \n",
    "    res = 0; n=len(w)\n",
    "    for i in range(n): res += mean_sqr_b(x[i]-x[i+n]) * w[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sr = Model([inp, vgg_inp], Lambda(content_fn)(vgg1+vgg2))\n",
    "targ = np.zeros((arr_hr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compile this chain of models and we can pass it the original low resolution image as well as the high resolution to train on. We also define a zero vector as a target parameter, which is a necessary parameter when calling fit on a keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 72, 72, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 72, 72, 64)    15616       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 72, 72, 64)    256         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 72, 72, 64)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 72, 72, 64)    36928       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 72, 72, 64)    256         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 72, 72, 64)    0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 72, 72, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 72, 72, 64)    256         convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 72, 72, 64)    0           batchnormalization_3[0][0]       \n",
      "                                                                   activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 72, 72, 64)    36928       merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 72, 72, 64)    256         convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 72, 72, 64)    0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 72, 72, 64)    36928       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNorma (None, 72, 72, 64)    256         convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 72, 72, 64)    0           batchnormalization_5[0][0]       \n",
      "                                                                   merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 72, 72, 64)    36928       merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_6 (BatchNorma (None, 72, 72, 64)    256         convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 72, 72, 64)    0           batchnormalization_6[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 72, 72, 64)    36928       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_7 (BatchNorma (None, 72, 72, 64)    256         convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 72, 72, 64)    0           batchnormalization_7[0][0]       \n",
      "                                                                   merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 72, 72, 64)    36928       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_8 (BatchNorma (None, 72, 72, 64)    256         convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 72, 72, 64)    0           batchnormalization_8[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 72, 72, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 72, 72, 64)    256         convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 72, 72, 64)    0           batchnormalization_9[0][0]       \n",
      "                                                                   merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_1 (UpSampling2D)    (None, 144, 144, 64)  0           merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 144, 144, 64)  36928       upsampling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 144, 144, 64)  256         convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 144, 144, 64)  0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_2 (UpSampling2D)    (None, 288, 288, 64)  0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 288, 288, 64)  36928       upsampling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (None, 288, 288, 64)  256         convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 288, 288, 64)  0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 288, 288, 3)   15555       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, Dimension(288) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 288, 288, 3)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  [(None, 288, 288, 64) 555328      input_2[0][0]                    \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (1, None)             0           model_1[1][0]                    \n",
      "                                                                   model_1[1][1]                    \n",
      "                                                                   model_1[1][2]                    \n",
      "                                                                   model_1[2][0]                    \n",
      "                                                                   model_1[2][1]                    \n",
      "                                                                   model_1[2][2]                    \n",
      "====================================================================================================\n",
      "Total params: 958,595\n",
      "Trainable params: 401,859\n",
      "Non-trainable params: 556,736\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m_sr.compile('adam', 'mse')\n",
    "m_sr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee3d4fc7d124db2878da476f032d47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebfd49668ee463cb8e4ee50ad4e489c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_sr.fit([arr_lr, arr_hr], targ, 8, 2, **parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.get_value(m_sr.optimizer.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use learning rate annealing to get a better fit.\n",
    "print(m_sr.optimizer.lr)\n",
    "K.set_value(m_sr.optimizer.lr, 1e-4)\n",
    "m_sr.fit([arr_lr, arr_hr], targ, 16, 1, **parms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
